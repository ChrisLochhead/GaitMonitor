{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb638b1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Programs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3004\\999011358.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import init_directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPrograms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mData_Processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel_Based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDemo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#from Programs.Data_Processing.Model_Based.Utilities import load, load_images, save_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrograms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mData_Processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel_Based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset_Creator\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mCreator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrograms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMachine_Learning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel_Based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutoEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGAE\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mGAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Programs'"
     ]
    }
   ],
   "source": [
    "#import init_directories\n",
    "from Programs.Data_Processing.Model_Based.Demo import *\n",
    "#from Programs.Data_Processing.Model_Based.Utilities import load, load_images, save_dataset\n",
    "import Programs.Data_Processing.Model_Based.Dataset_Creator as Creator\n",
    "import Programs.Machine_Learning.Model_Based.AutoEncoder.GAE as GAE\n",
    "import Programs.Machine_Learning.Model_Based.GCN.Dataset_Obj as Dataset_Obj\n",
    "import Programs.Machine_Learning.Model_Based.GCN.Ground_Truths as GT\n",
    "import Programs.Data_Processing.Model_Based.Render as Render\n",
    "import torch\n",
    "import torch_geometric\n",
    "import random\n",
    "import Programs.Machine_Learning.Model_Based.GCN.GAT as gat\n",
    "import Programs.Machine_Learning.Model_Based.GCN.STAGCN as stgcn\n",
    "import Programs.Machine_Learning.Model_Based.GCN.Utilities as graph_utils\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def process_data(folder = \"Chris\"):\n",
    "\n",
    "############################################# PIPELINE ##################################################################\n",
    "\n",
    "    #Extract joints from images\n",
    "    #run_images(\"./Code/Datasets/\" + str(folder) + \"/Full_Dataset\", out_folder=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/\", exclude_2D=False, \n",
    "    #           start_point=0)\n",
    "\n",
    "    \n",
    "    #Sort class labels (for 0ffice Images_chris this is 20-0, 20-1, 20-2)\n",
    "    #abs_joint_data = Creator.assign_class_labels(num_switches=20, num_classes=2, \n",
    "    #                                joint_file=\"./Code/Datasets/Joint_Data/WeightGait/Absolute_Data.csv\",\n",
    "    #                                joint_output=\"./Code/Datasets/Joint_Data/WeightGait/1_Absolute_Data(classes applied).csv\")\n",
    "    \n",
    "    #Display first 2 instances of results \n",
    "    #print(\"\\nStage 1: \")\n",
    "    #render_joints_series(\"./Code/Datasets/WeightGait/Raw_Images\", joints=abs_joint_data,\n",
    "    #                     size = 20, delay=True, use_depth=True)\n",
    "\n",
    "    #Remove empty frames\n",
    "    #abs_joint_data, image_data = Creator.process_empty_frames(joint_file=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/Absolute_Data.csv\",\n",
    "    #                                             image_file=\"./Code/Datasets/\" + str(folder) + \"/Full_Dataset/\",\n",
    "    #                                             joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/2_Absolute_Data(empty frames removed)\",\n",
    "    #                                             image_output=\"./Code/Datasets/\" + str(folder) + \"/2_Empty Frames Removed/\")\n",
    "\n",
    "    #Display first 2 instances of results\n",
    "    print(\"\\nStage 2: \")\n",
    "    #render_joints_series(image_data, abs_joint_data, size=15)\n",
    "    #render_joints_series(image_data, abs_joint_data, size=15, plot_3D=True)\n",
    "    \n",
    "    #Trim start and end frames where joints get confused by image borders\n",
    "    #abs_joint_data, image_data =Creator.process_trimmed_frames(abs_joint_data, image_data,\n",
    "    #                                                    joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/3_Absolute_Data(trimmed instances)\",\n",
    "    #                                                    image_output=\"./Code/Datasets/\" + str(folder) + \"/3_Trimmed Instances/\", trim = 5)\n",
    "\n",
    "    abs_joint_data, image_data = Utilities.process_data_input(\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/3_Absolute_Data(trimmed instances)/raw/3_Absolute_Data(trimmed instances).csv\",\n",
    "                                                              \"./Code/Datasets/\" + str(folder) + \"/3_Trimmed Instances/\", cols=Utilities.colnames, ignore_depth=False)\n",
    "    print(\"lens: \", len(abs_joint_data), len(image_data))\n",
    "    \n",
    "    #abs_norm_data = Creator.normalize_values(abs_joint_data, \n",
    "    #                                         joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/3_Absolute_Data(normed)\")\n",
    "    \n",
    "    abs_joint_data = Creator.new_normalize_values(abs_joint_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/3_Absolute_Data(normed)\", 3)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\nStage 3: \")\n",
    "    #render_joints_series(image_data, abs_joint_data, size=10)\n",
    "\n",
    "    #Normalize outliers\n",
    "    #SIMPLIFY\n",
    "    abs_joint_data = Creator.create_normalized_dataset(abs_joint_data, image_data, \n",
    "                                                   joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/4_Absolute_Data(normalized)\")\n",
    "    print(\"\\nStage 4:\")\n",
    "    #render_joints_series(image_data, abs_joint_data, size=10)\n",
    "\n",
    "    abs_joint_data = Creator.append_midhip(abs_joint_data, image_data, \n",
    "                                                   joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/4.5_Absolute_Data(midhip)\")\n",
    "     \n",
    "    print(\"\\nStage 4.5:\")\n",
    "    #render_joints_series(image_data, abs_joint_data, size=10)\n",
    "    #Change format of pre-scale to list of arrays instead of list of lists\n",
    "    #pre_scale = abs_joint_data\n",
    "\n",
    "    #Normalize size (use absolute dataset)\n",
    "    abs_joint_data = Creator.create_scaled_dataset(abs_joint_data, image_data,\n",
    "                                               joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/5_Absolute_Data(scaled)\")\n",
    "\n",
    "    print(\"\\nStage 5: \")\n",
    "    #render_joints_series(image_data, abs_joint_data, size=10)\n",
    "\n",
    "    #Create relative dataset\n",
    "    relative_joint_data = Creator.create_relative_dataset(abs_joint_data, image_data,\n",
    "                                                 joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/6_Relative_Data(relative)\")\n",
    "    \n",
    "    print(\"\\nStage 6:\")\n",
    "    #render_joints_series(\"None\", relative_joint_data, size=5, plot_3D=True, x_rot = -90, y_rot = 180)\n",
    "\n",
    "    #Flip joints so all facing one direction\n",
    "    relative_joint_data = Creator.create_flipped_joint_dataset(relative_joint_data, abs_joint_data, image_data,\n",
    "                                                               joint_output = \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/7_Relative_Data(flipped)\") \n",
    "\n",
    "    print(\"\\nStage 7:\")\n",
    "    #render_joints_series(\"None\", flipped_joint_data, size=5, plot_3D=True, x_rot = -90, y_rot = 180)\n",
    "\n",
    "    #Create velocity dataset\n",
    "    velocity_data = Creator.create_velocity_dataset(abs_joint_data, image_data, \n",
    "                                                    joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/8_Velocity_Data(velocity)\")\n",
    "    print(\"\\nStage 8:\")\n",
    "    #render_velocity_series(abs_joint_data, velocity_data, image_data, size=20)\n",
    "\n",
    "    velocity_data = Creator.create_flipped_joint_dataset(velocity_data, abs_joint_data, image_data,\n",
    "                                                               joint_output = \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/9_Velocity_Data(flipped)\")  \n",
    "\n",
    "    print(\"\\nStage 9: \")\n",
    "    #Create joint angles data\n",
    "    joint_bones_data = Creator.create_bone_dataset(abs_joint_data, image_data, \n",
    "                                                   joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/10_Bones_Data(integrated)\")\n",
    "    \n",
    "    #render_velocity_series(abs_joint_data, joint_bones_data, image_data, size=20)\n",
    "\n",
    "    print(\"\\nStage 10:\")\n",
    "    #Create regions data\n",
    "    top_region_dataset, bottom_region_dataset = Creator.create_2_regions_dataset(abs_joint_data, #CHANGE BACK TO ABS_JOINT_DATA\n",
    "                                                                                 joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/11_2_Region_Data\",\n",
    "                                                                                 images = image_data)\n",
    "    regions_data = Creator.create_5_regions_dataset(abs_joint_data,\n",
    "                                                    \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/12_5_Data_\",\n",
    "                                                      image_data)\n",
    "    \n",
    "    regions_data = Creator.create_decimated_dataset(abs_joint_data,\n",
    "                                                \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/12_75_no_head_Data_\",\n",
    "                                                    image_data)\n",
    "    #print(\"\\nStage 11:\", pre_scale[0])\n",
    "    #Create HCF dataset\n",
    "    #hcf_data = Creator.create_hcf_dataset(pre_scale, abs_joint_data, relative_joint_data, velocity_data, image_data, \n",
    "    #                           joints_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/13_HCF_Data\")\n",
    "    \n",
    "    #hcf_data_normed = Creator.normalize_values(hcf_data, \n",
    "    #                                         joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/13.5_HCF_Data(normed)\", hcf=True)\n",
    "    \n",
    "    print(\"\\nStage 12:\")\n",
    "    #Create ground truth comparison set\n",
    "    #print(\"data going into gait cycle extractor: \", pre_scale[0])\n",
    "    #ground_truths = Creator.create_ground_truth_dataset(pre_scale, abs_joint_data, relative_joint_data, velocity_data, hcf_data, image_data,\n",
    "    #                                                    joints_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/14_Ground_Truth_Data\")\n",
    "    \n",
    "    print(\"\\nStage 13:\")\n",
    "    #Combine datasets (relative, velocity, joint angles, regions)\n",
    "    combined_data = Creator.combine_datasets(relative_joint_data, velocity_data, joint_bones_data, image_data,\n",
    "                                             joints_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/15_Combined_Data\")\n",
    "    \n",
    "    #combined_norm_data = Creator.normalize_values(combined_data, \n",
    "    #                                         joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/15.5_Combined_Data(normed)\")\n",
    "    \n",
    "    #combined_norm_data = Creator.new_normalize_values(combined_data, \n",
    "    #                                         \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/15.5_Combined_Data(normed)\", 9)   \n",
    "    combined_norm_data = combined_data\n",
    "    fake_data = Creator.create_dummy_dataset(combined_norm_data, output_name=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/0_Dummy_Data_15.5\")\n",
    "    \n",
    "    print(\"\\nStage 14:\")\n",
    "    #Create regions data of combined data\n",
    "    top_region_dataset, bottom_region_dataset = Creator.create_2_regions_dataset(combined_norm_data, #CHANGE BACK TO ABS_JOINT_DATA\n",
    "                                                                                 joint_output=\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/16_Combined_Data_2Region\",\n",
    "                                                                                 images = image_data)\n",
    "    regions_data = Creator.create_5_regions_dataset(combined_norm_data,\n",
    "                                                    \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/17_Combined_Data_5Region\",\n",
    "                                                      image_data)\n",
    "    \n",
    "    fused_data = Creator.create_fused_dataset(combined_norm_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/18_Combined_Data_Fused\")\n",
    "\n",
    "    precise_data = Creator.normal_examples_only(combined_norm_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/19_Normal_Only\" )\n",
    "    \n",
    "    if folder == \"WeightGait\":\n",
    "        full_data, _ = Utilities.process_data_input(\"./Code/Datasets/Joint_Data/\" + str(folder) + \"/15_Combined_Data/raw/15_Combined_Data.csv\",\n",
    "                                                              \"./Code/Datasets/\" + str(folder) + \"/15_Combined_Data/\", cols=Utilities.colnames_midhip, ignore_depth=False)\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_1_People\", [1])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_2_People\", [0,1])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_3_People\", [0,1,2])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_4_People\", [0,3,5,6])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_Cade\", [2])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_Elisa\", [4])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_Longfei\", [5])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_Pheobe\", [6])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_SeanC\", [7])\n",
    "        two_person_data = Creator.create_n_size_dataset(full_data, \"./Code/Datasets/Joint_Data/\" + str(folder) + \"/20_SeanG\", [8])\n",
    "#########################################################################################################################\n",
    "\n",
    "def load_2_region_data(folder = \"Chris\"):\n",
    "    top_region = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/16_Combined_Data_2Region_top',\n",
    "                                        '16_Combined_Data_2Region_top.csv',\n",
    "                                        joint_connections=Render.joint_connections_m_hip, cycles=True)\n",
    "    \n",
    "    bottom_region = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/16_Combined_Data_2Region_bottom',\n",
    "                                        '16_Combined_Data_2Region_bottom.csv',\n",
    "                                        joint_connections=Render.joint_connections_m_hip, cycles=True)\n",
    "           \n",
    "    return top_region, bottom_region\n",
    "\n",
    "def load_5_region_data(folder = \"Chris\"):\n",
    "    left_leg = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/17_Combined_Data_5Regionl_leg',\n",
    "                                              '17_Combined_Data_5Regionl_leg.csv',\n",
    "                                              joint_connections=Render.limb_connections, cycles=True)\n",
    "    \n",
    "    right_leg = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/17_Combined_Data_5Regionr_leg',\n",
    "                                           '17_Combined_Data_5Regionr_leg.csv',\n",
    "                                            joint_connections=Render.limb_connections, cycles=True)\n",
    "    left_arm = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/17_Combined_Data_5Regionl_arm',\n",
    "                                              '17_Combined_Data_5Regionl_arm.csv',\n",
    "                                              joint_connections=Render.limb_connections, cycles=True)\n",
    "    \n",
    "    right_arm = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/17_Combined_Data_5Regionr_arm',\n",
    "                                           '17_Combined_Data_5Regionr_arm.csv',\n",
    "                                            joint_connections=Render.limb_connections, cycles=True)   \n",
    "    head_data = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/17_Combined_Data_5Regionhead',\n",
    "                                              '17_Combined_Data_5Regionhead.csv',\n",
    "                                              joint_connections=Render.head_joint_connections, cycles=True)\n",
    "\n",
    "    return left_leg, right_leg, left_arm, right_arm, head_data\n",
    "\n",
    "#Types are 1 = normal, 2 = HCF, 3 = 2 region, 4 = 5 region, 5 = Dummy. Pass types as an array of type numbers, always put hcf (2) at the END if including. If including HCF, you MUST include\n",
    "#as cycles = True\n",
    "def load_datasets(types, folder):\n",
    "    datasets = []\n",
    "    print(\"loading datasets...\")\n",
    "        \n",
    "    for i, t in enumerate(types):\n",
    "        print(\"loading dataset {} of {}. \".format(i + 1, len(types)), t)\n",
    "        #Type 1: Normal, full dataset\n",
    "        if t == 1:  \n",
    "            #15.5 COMBINED DATASET\n",
    "            #datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/15_Combined_Data', '15_Combined_Data.csv',\n",
    "            #                                        joint_connections=Render.joint_connections_m_hip, cycles=True))\n",
    "            \n",
    "            #20 multi person DATASET\n",
    "            #datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/20_4_people', '20_4_people.csv',\n",
    "            #                                       joint_connections=Render.joint_connections_m_hip, cycles=True))\n",
    "            \n",
    "            #7 co-ordinates on their own\n",
    "            datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/7_Relative_Data(flipped)', '7_Relative_Data(flipped).csv',\n",
    "                                                    joint_connections=Render.joint_connections_m_hip, cycles=True))\n",
    "\n",
    "            #19 simplified dataset\n",
    "            #datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/19_Normal_Only', '19_Normal_Only.csv',\n",
    "            #                                        joint_connections=Render.joint_connections_no_head_m_hip, cycles=True))\n",
    "            \n",
    "            #datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/0_Dummy_Data_15.5',\n",
    "            #                                          '0_Dummy_Data_15.5.csv',\n",
    "             #                                       joint_connections=Render.joint_connections_m_hip, cycles=True))\n",
    "            \n",
    "        elif t == 1:\n",
    "            dataset = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/15.5_Combined_Data(normed)', '15.5_Combined_Data(normed).csv',\n",
    "                                        joint_connections=Render.joint_connections_m_hip, cycles=False)\n",
    "            datasets.append(dataset)\n",
    "        #Type 2: HCF dataset\n",
    "        elif t == 2:\n",
    "            #This MUST have cycles, there's no non-cycles option\n",
    "            dataset = Dataset_Obj.HCFDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/13.5_HCF_Data(normed)',\n",
    "                                                    '13.5_HCF_Data(normed).csv', cycles=True)\n",
    "            datasets.append(dataset)\n",
    "        #Type 3: 2 region\n",
    "        elif t == 3:\n",
    "            top_region, bottom_region = load_2_region_data(folder)\n",
    "            datasets.append(top_region)\n",
    "            datasets.append(bottom_region)\n",
    "        #Type 4: 5 region\n",
    "        elif t == 4:\n",
    "            l_l, r_l, l_a, r_a, h = load_5_region_data(folder)\n",
    "            datasets.append(l_l)\n",
    "            datasets.append(r_l)\n",
    "            datasets.append(l_a)\n",
    "            datasets.append(r_a)\n",
    "            datasets.append(h)\n",
    "        #Type 5: Dataset with dummy datapoints\n",
    "        elif t == 5:\n",
    "            #Dummy dataset\n",
    "            datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/0_Dummy_Data_15.5',\n",
    "                                                      '0_Dummy_Data_15.5.csv',\n",
    "                                                    joint_connections=Render.joint_connections_m_hip, cycles=True))\n",
    "            \n",
    "            #15.5 COMBINED DATASET (original version of the dummy dataset)\n",
    "            datasets.append(Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/15_Combined_Data', '15_Combined_Data.csv',\n",
    "                                                    joint_connections=Render.joint_connections_m_hip, cycles=True))                        \n",
    "\n",
    "    print(\"datasets loaded.\")\n",
    "    #Return requested datasets\n",
    "    return datasets\n",
    "\n",
    "def get_balanced_samples(dataset, train = 0.9, test = 0.1):\n",
    "    print(len(dataset))\n",
    "    class_size = int(len(dataset) / 3)\n",
    "    print(\"class size: \", class_size)\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    train_class_size = int(class_size * 0.9)\n",
    "    to_fill = [train_class_size, train_class_size, train_class_size]\n",
    "    print(\"fill sizes: \", to_fill)\n",
    "    for i, cycle in enumerate(dataset):\n",
    "        #print(\"cycle: \", cycle.y.item())\n",
    "        if to_fill[cycle.y.item()] > 0:\n",
    "            to_fill[cycle.y.item()] -= 1\n",
    "            train_indices.append(i)\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        if i not in train_indices:\n",
    "            test_indices.append(i)\n",
    "\n",
    "    print(\"to fill should be empty: \", to_fill)\n",
    "    print(\"train indices: \", len(dataset), len(train_indices))\n",
    "    print(\"len test: \", len(test_indices))\n",
    "\n",
    "    print(\"indices: \", train_indices)\n",
    "    print(\"test: \", test_indices)\n",
    "\n",
    "    print(\"done\")\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "def process_datasets(datasets):\n",
    "    print(\"Processing data...\")\n",
    "\n",
    "    train_indice_list = []\n",
    "    test_indice_list = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_size = len(dataset)\n",
    "\n",
    "        #Append indices based on the first dataset length\n",
    "        train_indices = random.sample(range(dataset_size), int(0.7 * dataset_size))\n",
    "        print(\"original indices:\", len(train_indices), train_indices )\n",
    "        test_indices = random.sample(set(range(dataset_size)) - set(train_indices), int(0.3 * dataset_size))\n",
    "        print(\"original test indices:\", len(test_indices), test_indices )\n",
    "        #train_indices, test_indices = get_balanced_samples(datasets[0])\n",
    "        #done = 5/0\n",
    "        train_indice_list.append(train_indices)\n",
    "        test_indice_list.append(test_indices)\n",
    "\n",
    "    #These regions will be the same for both datasets\n",
    "    multi_input_train_val = []\n",
    "    multi_input_test = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        multi_input_train_val.append(dataset[train_indice_list[i]])\n",
    "        multi_input_test.append(dataset[test_indice_list[i]])\n",
    "    \n",
    "    print(\"Dataset processing complete.\")\n",
    "\n",
    "    return multi_input_train_val, multi_input_test\n",
    "\n",
    "def process_results(train_scores, val_scores, test_scores):\n",
    "    print(\"Final results: \")\n",
    "    for ind, t in enumerate(test_scores):\n",
    "        test_scores[ind] = test_scores[ind].cpu()\n",
    "        test_scores[ind] = float(test_scores[ind])\n",
    "\n",
    "    for i, score in enumerate(train_scores):\n",
    "        print(\"score {:.2f}: training: {:.2f}%, validation: {:.2f}%, test: {:.2f}%\".format(i, score * 100, val_scores[i] * 100, test_scores[i] * 100))\n",
    "\n",
    "    mean, var = Utilities.mean_var(test_scores)\n",
    "    print(\"mean, std and variance: {:.2f}%, {:.2f}% {:.5f}\".format(mean, math.sqrt(var), var))\n",
    "\n",
    "def process_autoencoder(folder, num_epochs, batch_size):\n",
    "    #load dataset\n",
    "    dataset = Dataset_Obj.JointDataset('./Code/Datasets/Joint_Data/' + str(folder) + '/15.5_Combined_Data(normed)', '15.5_Combined_Data(normed).csv',\n",
    "                                            joint_connections=Render.joint_connections_m_hip, cycles=True)             \n",
    "\n",
    "    name = \"15.5_Combined_Data(normed)\"\n",
    "\n",
    "    vae = GAE.VariationalAutoencoder(dim_in=dataset.num_nodes_per_graph, dim_h=128, latent_dims=3, batch_size=batch_size, cycle_size=dataset.max_cycle)\n",
    "    optim = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=1e-5) # originally 5\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "    for epoch in range(num_epochs):\n",
    "        _, _, _, whole = GT.create_dataloaders(dataset, batch_size=batch_size)\n",
    "        train_val_data, test_data = process_datasets([dataset])\n",
    "        train_loader, val_loader, test_loader = graph_utils.cross_valid(None, test_data, datasets=train_val_data, make_loaders=True, batch=batch_size)\n",
    "\n",
    "        train_loss = GAE.train_epoch(vae,device,train_loader[0],optim)\n",
    "        #Get embedding of entire dataset and save it\n",
    "        val_loss = GAE.test_epoch(vae,device,whole, './Code/Datasets/Joint_Data/WeightGait/' + str(name) + '/encoded/raw/encoded.csv', \n",
    "                                skeleton_size = 21)\n",
    "        print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "\n",
    "\n",
    "    print(\"Autoencoding complete\")\n",
    "\n",
    "    #Load autoencoded model (data will be 16 * 3 * 71)\n",
    "\n",
    "    #Compress\n",
    "\n",
    "def run_model(dataset_types, model_type, hcf, batch_size, epochs, folder, leave_one_out):\n",
    "\n",
    "    #Load the full dataset alongside HCF with gait cycles\n",
    "    datasets = load_datasets(dataset_types, folder)\n",
    "    print(\"datasets here: \", datasets)\n",
    "    #Concatenate data dimensions for ST-GCN\n",
    "    data_dims = []\n",
    "    for dataset in datasets:\n",
    "        data_pair = [dataset.num_features, dataset.num_node_features]\n",
    "        data_dims.append(data_pair)\n",
    "\n",
    "    num_datasets = len(datasets)\n",
    "\n",
    "    #Accounting for extra original dataset not used in dummy case for training but only for testing\n",
    "    if 5 in dataset_types:\n",
    "        num_datasets -= 1\n",
    "\n",
    "    print(\"number of datasets: \", num_datasets)\n",
    "    \n",
    "    #Split classes by just making the last person the test set and the rest training and validation.\n",
    "    if leave_one_out:\n",
    "        multi_input_train_val, multi_input_test = graph_utils.split_data_by_person(datasets)\n",
    "    else:\n",
    "        #Process datasets by manually shuffling to account for cycles\n",
    "        multi_input_train_val, multi_input_test = process_datasets(datasets)\n",
    "\n",
    "    dim_out = 3\n",
    "\n",
    "    print(\"\\nCreating {} model with {} datasets: \".format(model_type, len(datasets)))\n",
    "    if model_type == \"GAT\":\n",
    "        model = gat.MultiInputGAT(dim_in=[d.num_node_features for d in datasets], dim_h=64, dim_out=dim_out, hcf=hcf, n_inputs=num_datasets)\n",
    "    elif model_type == \"ST-AGCN\":\n",
    "        print(\"going in: \", datasets[0].num_node_features)\n",
    "        model = stgcn.MultiInputSTGACN(dim_in=[d.num_node_features for d in datasets], dim_h=32, num_classes=dim_out, n_inputs=num_datasets,\n",
    "                                    data_dims=data_dims, batch_size=batch_size, hcf=hcf,\n",
    "                                    max_cycle=datasets[0].max_cycle, num_nodes_per_graph=datasets[0].num_nodes_per_graph)\n",
    "    else:\n",
    "        print(\"Invalid model type.\")\n",
    "        return\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    train_scores, val_scores, test_scores = graph_utils.cross_valid(model, multi_input_test, datasets=multi_input_train_val,\n",
    "                                                                     k_fold=4, batch=batch_size, epochs=epochs, type=model_type)\n",
    "\n",
    "    #Process and display results\n",
    "    process_results(train_scores, val_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #process_data(\"Chris\")\n",
    "    #process_autoencoder(\"Elisa\", 100, 8)\n",
    "\n",
    "    #Run the model:\n",
    "    #Dataset types: Array of types for the datasets you want to pass through at the same time\n",
    "    #   1: normal full body 9D dataset\n",
    "    #   2: HCF data\n",
    "    #   3: 2 region data\n",
    "    #   4: 5 region data\n",
    "    #Cycles: is the data passed through as single frames or gait cycles\n",
    "    #Model type: \"ST-AGCN\" or \"GAT\"\n",
    "    #hcf: indicates presence of an HCF dataset\n",
    "    #multi: indicates if the multi-stream variant of the chosen model should be used (multi variant \n",
    "    # models are compatible with both single and multiple datasets)\n",
    "    #Leave_one_out: indicates whether using normally split data or data split by person\n",
    "\n",
    "    run_model(dataset_types= [1], model_type = \"ST-AGCN\", hcf=False,\n",
    "           batch_size = 16, epochs = 150, folder=\"WeightGait\", leave_one_out=False)\n",
    "\n",
    "    #Full datset can overfit at 130 - 180 epochs at 8 batch size\n",
    "    #Full dataset at 16 overfits at <100 epochs\n",
    "\n",
    "    #Things to try:\n",
    "    # 78% batch 8 epochs 150 4 fold with just Chris\n",
    "\n",
    "\n",
    "    #Full dataset one person only (person 0) : 61%\n",
    "    #Bob = random (32%)\n",
    "    #Cade = worse than random (25%)\n",
    "    #Elisa = random (35%)\n",
    "    #Longfei = decent (55%)\n",
    "    #Pheobe = decent (62%)\n",
    "    #Sean C = less than decent (40%)\n",
    "    #Sean G = random (37%, 52%, 65%, 50%) 40 epochs: (62%, 50%)\n",
    "    #Two people (0, 1) = random\n",
    "    #4 best people gives 57% tiny variance\n",
    "\n",
    "    # try 1,2,3,4 with multiple datasets (need to implement code for that)\n",
    "    #Find where it just stops working on valid, need bigger network probably (4 layers, 128,128,256,256 is what paper used\n",
    "    # Test on telling people apart as the class\n",
    "\n",
    "\n",
    "    #BOBS SUGGESTIONS\n",
    "    #reassess all data manually to find problem instances\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
